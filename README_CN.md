<p align="left">
    ä¸­æ–‡ | <a href="README.md">English</a>
</p>

<p align="center">
    <img src="data/chat.jpg" alt="Tiny Qwen äº¤äº’å¼å¯¹è¯">
</p>

# âœ¨ Tiny Qwen

ä¸€ä¸ªç®€æ´æ˜“è¯»çš„ PyTorch ä»£ç åº“ï¼Œç”¨äºé‡æ–°å®ç° `Qwen3` å’Œ `Qwen2.5-VL`ï¼Œæ”¯æŒçº¯æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ï¼Œä»¥åŠç¨ å¯†å’Œæ··åˆä¸“å®¶æ¶æ„ã€‚

å¦‚æœä½ è§‰å¾— Hugging Face ä»£ç è¿‡äºå†—é•¿éš¾æ‡‚ï¼Œè¿™ä¸ªåº“æ­£é€‚åˆä½ ï¼

æ¬¢è¿å¤§å®¶åŠ æˆ‘çš„ [Discord ç¾¤](https://discord.gg/sBNnqP9gaY)ç»§ç»­è®¨è®ºï¼

## ğŸ¦‹ å¿«é€Ÿå¼€å§‹

æ¨èä½¿ç”¨ `uv` åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š

```bash
pip install uv && uv venv

# æ¿€æ´»ç¯å¢ƒ
source .venv/bin/activate # Linux/macOS
.venv\Scripts\activate # Windows

# å®‰è£…ä¾èµ–
uv pip install -r requirements.txt
```

å¯åŠ¨äº¤äº’å¼å¯¹è¯ï¼š

```bash
python run.py
```

**æ³¨æ„ï¼š** `Qwen3` ä»…æ”¯æŒæ–‡æœ¬ã€‚ä½¿ç”¨ `@path/to/image.jpg` ä¸º `Qwen2.5-VL` å¼•ç”¨å›¾ç‰‡ã€‚

```
USER: @data/test-img-1.jpg å‘Šè¯‰æˆ‘è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ
âœ“ Found image: data/test-img-1.jpg
ASSISTANT: è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†å……æ»¡æ´»åŠ›çš„å‘æ—¥è‘µç”°...
```

## ğŸ“ ä»£ç ç¤ºä¾‹

**è¿è¡Œ `Qwen2.5-VL`ï¼š**

```python
from PIL import Image
from model.model import Qwen2VL
from model.processor import Processor

model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
model = Qwen2VL.from_pretrained(repo_id=model_name, device_map="auto")
processor = Processor(repo_id=model_name, vision_config=model.config.vision_config)

context = [
    "<|im_start|>user\n<|vision_start|>",
    Image.open("data/test-img-1.jpg"),
    "<|vision_end|>è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ<|im_end|>\n<|im_start|>assistant\n",
]

inputs = processor(context, device="cuda")

generator = model.generate(
    input_ids=inputs["input_ids"],
    pixels=inputs["pixels"],
    d_image=inputs["d_image"],
    max_new_tokens=64,
    stream=True,
)

for token_id in generator:
    token_text = processor.tokenizer.decode([token_id])
    print(token_text, end="", flush=True)
print()
```

**è¿è¡Œ `Qwen3`ï¼š**

```python
from model.model import Qwen3MoE
from model.processor import Processor

model_name = "Qwen/Qwen3-4B-Instruct-2507"
model = Qwen3MoE.from_pretrained(repo_id=model_name)
processor = Processor(repo_id=model_name)

context = [
    "<|im_start|>user\n<|vision_start|>",
    "<|vision_end|>è§£é‡Šä¸€ä¸‹åè½¬é“¾è¡¨<|im_end|>\n<|im_start|>assistant\n",
]
inputs = processor(context, device="cuda")
generator = model.generate(
    input_ids=inputs["input_ids"],
    max_new_tokens=64,
    stream=True
)

for token_id in generator:
    token_text = processor.tokenizer.decode([token_id])
    print(token_text, end="", flush=True)
print()
```
